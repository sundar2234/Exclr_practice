{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32d0dcf-922d-48ac-b411-8d79a48d7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import random \n",
    "import string # to process standard python strings \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfc1652-240d-4340-99d6-e6d1962098ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ffd2b1-4d96-46ba-a412-9c3d2aca6e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bknai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bknai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular', quiet=True) \n",
    "nltk.download('punkt')  \n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8742e44e-1bc4-4fd1-a49a-ed3939c9a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('input.txt','r', errors = 'ignore') \n",
    "raw = f.read() \n",
    "raw = raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc9f13c-a297-465c-8edd-1f76bff28907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e5bb826-d25e-457b-bc90-d3e7bba15988",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2363051983.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    return Lem Tokens(nltk.word_tokenize(text.lower().translate(remove_p unct_dict)))\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer() \n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK. \n",
    "def LemTokens(tokens): \n",
    "    return [lemmer.lemmatize(token) for token in tokens] \n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) \n",
    "def LemNormalize(text): \n",
    "    return Lem Tokens(nltk.word_tokenize(text.lower().translate(remove_p unct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fc5eb3a-de19-4d76-81fe-e7030949fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bknai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bknai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'faster', 'than', 'the', 'fastest', 'runner']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# Example usage\n",
    "text = \"Running faster than the fastest runner!\"\n",
    "print(LemNormalize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fde84e5-5589-436b-b662-66d21de837c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'faster', 'than', 'the', 'fastest', 'runner']\n",
      "this is a sample input file.\n",
      "it contains multiple lines of text.\n",
      "you can modify it as needed.\n",
      "java and python are popular programming languages.\n",
      "machine learning and ai are evolving rapidly.\n",
      "technology is advancing at a fast pace.\n",
      "data science is a growing field.\n",
      "cloud computing enables scalable applications.\n",
      "cybersecurity is essential for online safety.\n",
      "automation is transforming industries.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('popular', quiet=True)\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Dictionary to remove punctuation\n",
    "remove_punct_dict = {ord(punct): None for punct in string.punctuation}\n",
    "\n",
    "# Function to normalize text (lowercase, remove punctuation, tokenize, lemmatize)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# Read input file\n",
    "with open('input.txt', 'r', errors='ignore') as f:\n",
    "    raw = f.read().lower()\n",
    "\n",
    "# Tokenization\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "# Example usage\n",
    "print(LemNormalize(\"Running faster than the fastest runner!\"))\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86a818e1-ee9f-449e-bc5a-987fbbb0cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"what's up\",\"hey\",\"how are you?\") \n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\",\"I am glad! You are talking to me\", \"I am fine! How about you?\"] \n",
    "def greeting(sentence): \n",
    "    for word in sentence.split(): \n",
    "        if word.lower() in GREETING_INPUTS: \n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e29eece4-d0bb-4674-b302-342cdcc15575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response): \n",
    "    robo_response=' ' \n",
    "    sent_tokens.append(user_response) \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') \n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens) \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf) \n",
    "    idx=vals.argsort()[0][-2] \n",
    "    flat = vals.flatten() \n",
    "    flat.sort() \n",
    "    req_tfidf = flat[-2] \n",
    "    if(req_tfidf==0): \n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you.\" \n",
    "        return robo_response \n",
    "    else: \n",
    "        robo_response = robo_response+sent_tokens[idx] \n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81c77a6e-1996-4e35-9d20-d57a482b3407",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1605242270.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[33], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    if(user_response! = 'bye'):\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "flag=True \n",
    "print(\"SABot: My name is SABot. How can I assist you?. If you want to exit, type Bye!\") \n",
    "while(flag==True): \n",
    "    user_response = input() \n",
    "    user_response=user_response.lower() \n",
    "    if(user_response! = 'bye'): \n",
    "        if(user_response == 'thanks' or user_response == 'thank you' ): \n",
    "            flag=False \n",
    "            print(\"SABot: You are welcome...\") \n",
    "        else: \n",
    "            if(greeting(user_response)!=None): \n",
    "            print(\"SABot: \"+greeting(user_response)) \n",
    "    else: \n",
    "        print(\"SABot: \",end=\" \") \n",
    "        print(response(user_response)) \n",
    "        sent_tokens.remove(user_response) \n",
    "    else: \n",
    "        flag=False \n",
    "        print(\"SABot: Bye! take care...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6efb9343-7c47-4f9c-b5d3-71fd5148a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SABot: My name is SABot. How can I assist you? If you want to exit, type Bye!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SABot: I am glad! You are talking to me\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " welcome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SABot:   I am sorry! I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SABot: Bye! Take care...\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"SABot: My name is SABot. How can I assist you? If you want to exit, type Bye!\") \n",
    "\n",
    "while flag:\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "\n",
    "    if user_response != 'bye': \n",
    "        if user_response == 'thanks' or user_response == 'thank you': \n",
    "            flag = False\n",
    "            print(\"SABot: You are welcome...\") \n",
    "        else: \n",
    "            if greeting(user_response) is not None:  # Ensure `greeting()` is defined\n",
    "                print(\"SABot: \" + greeting(user_response))  \n",
    "            else:\n",
    "                print(\"SABot: \", end=\" \")\n",
    "                print(response(user_response))  # Ensure `response()` is defined\n",
    "    else: \n",
    "        flag = False\n",
    "        print(\"SABot: Bye! Take care...\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b1e1c-a3b2-4ec4-b963-fd0cefda5008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
